{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-04-20T10:51:21.085780Z",
     "iopub.execute_input": "2023-04-20T10:51:21.086216Z",
     "iopub.status.idle": "2023-04-20T10:51:21.118635Z",
     "shell.execute_reply.started": "2023-04-20T10:51:21.086177Z",
     "shell.execute_reply": "2023-04-20T10:51:21.117621Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook shows how a neural network works theoretically from a mathematics point of view. The code here is not optimal, and is intentionally developed this way so that the reader understands the mechanism behind the weight updating logic of a neural network.\n",
    "The model here is of a simple polynomial, whose degree can be set by the user. The model will then learn the weights required to produce the required output for the given input, using the gradient descent method.\n",
    "\n",
    "The theory is that the weights are updated in the opposite direction of the gradient of the cost function. A gradient is a first derivative of a given function, which indicates the slope of the function. When the weights are updated in the opposite direction, the cost reduces. The objective is to reduce the cost to an acceptable limit. \n",
    "\n",
    "The cost described above is the error of the prediction. In this case, Mean Square Error is used as the cost function. The gradient then becomes the 1st derivative of the MSE with respect to each of the weights. Each of the weights will be updated in the opposite direction of the gradient of the cost with respect to itself."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class MyPoly:\n",
    "    def __init__(self, degree, learning_rate):\n",
    "        self.degree = degree\n",
    "        degree += 1\n",
    "        random_boundary = 1 / np.sqrt(degree)\n",
    "        self.W = np.random.uniform(low=-random_boundary, high=random_boundary, size=self.degree)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def summary(self):\n",
    "        print(f\"Degree : {self.degree}\")\n",
    "        print(f\"Learning rate : {self.learning_rate}\")\n",
    "        print(f\"Weights : {self.W[1:]}\")\n",
    "        print(f\"Bias : {self.W[0]}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_powers = np.asanyarray([np.power(x, power) for power in range(self.degree)])\n",
    "        res = np.sum(np.multiply(self.W, x_powers))\n",
    "        return res\n",
    "\n",
    "    def get_cost(self, y_pred, y):\n",
    "        error = self.get_losses(y_pred, y)\n",
    "        error = np.square(error)\n",
    "        loss = np.sum(error) / len(y)\n",
    "        return loss\n",
    "\n",
    "    def get_losses(self, y_pred, y):\n",
    "        return y - y_pred\n",
    "\n",
    "    def update_weights(self, inputs, outputs):\n",
    "        N = len(inputs)\n",
    "        for i in range(self.degree):\n",
    "            dc = 0\n",
    "            for j in range(N):\n",
    "                xj = inputs[j]\n",
    "                dc += np.multiply(np.power(xj, i), (outputs[j] - self.compute([xj])))\n",
    "            self.W[i] -= self.learning_rate * (-dc * 2 / N)\n",
    "\n",
    "    def compute(self, inputs):\n",
    "        T = len(inputs)\n",
    "        return np.asarray([self.forward(inputs[t]) for t in range(T)])\n",
    "\n",
    "    def fit(self, inputs, outputs, epochs=100000):\n",
    "        costs = []\n",
    "        for epoch in range(epochs):\n",
    "            show_cost = epoch % 1000 == 0\n",
    "            self.update_weights(inputs, outputs)\n",
    "            if show_cost:\n",
    "                pred = self.compute(inputs)\n",
    "                new_cost = self.get_cost(outputs, pred)\n",
    "                print(f\"\\nIteration {epoch} - Cost : {new_cost}\")\n",
    "                print(f\"Current weights : {[float('%.2f' % elem) for elem in model.W]}\")\n",
    "                print(f\"Current results : {[float('%.2f' % elem) for elem in pred]}\")\n",
    "                print(f\"Required results : {[float('%.2f' % elem) for elem in outputs]}\")\n",
    "                costs.append(new_cost)\n",
    "        return costs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-20T10:51:21.120739Z",
     "iopub.execute_input": "2023-04-20T10:51:21.121410Z",
     "iopub.status.idle": "2023-04-20T10:51:21.135769Z",
     "shell.execute_reply.started": "2023-04-20T10:51:21.121370Z",
     "shell.execute_reply": "2023-04-20T10:51:21.134658Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = MyPoly(5, 0.000019)\n",
    "model.summary()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-20T10:51:21.137541Z",
     "iopub.execute_input": "2023-04-20T10:51:21.138247Z",
     "iopub.status.idle": "2023-04-20T10:51:21.155979Z",
     "shell.execute_reply.started": "2023-04-20T10:51:21.138209Z",
     "shell.execute_reply": "2023-04-20T10:51:21.154506Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "Degree : 5\nLearning rate : 1.9e-05\nWeights : [-0.39711963 -0.16815389 -0.00114842  0.34717215]\nBias : 0.3076202935911398\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_x = [-3.5, -3.25, -3, -2.75, -2.5, -2.25, -2, -1.75, -1.5, -1.25, -1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1,\n",
    "           1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75]\n",
    "train_y = [297.40625, 243.4550781, 194.5, 153.1738281, 118.65625, 90.17382813, 67, 48.45507813, 33.90625, 22.76757813,\n",
    "           14.5, 8.611328125, 4.65625, 2.236328125, 1, 0.642578125, 0.90625, 1.580078125, 2.5, 3.548828125, 4.65625,\n",
    "           5.798828125, 7, 8.330078125, 9.90625, 11.89257813, 14.5, 17.98632813, 22.65625, 28.86132813]\n",
    "\n",
    "model.fit(train_x, train_y)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-20T10:51:21.158111Z",
     "iopub.execute_input": "2023-04-20T10:51:21.158509Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "\nIteration 0 - Cost : 5316.476595382025\nCurrent weights : [0.31, -0.4, -0.16, -0.04, 0.44]\nCurrent results : [67.86, 50.67, 37.0, 26.35, 18.24, 12.21, 7.88, 4.9, 2.93, 1.72, 1.04, 0.68, 0.5, 0.4, 0.31, 0.2, 0.09, 0.04, 0.15, 0.56, 1.45, 3.04, 5.6, 9.45, 14.91, 22.4, 32.32, 45.17, 61.45, 81.71]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 1000 - Cost : 132.59752336786516\nCurrent weights : [0.61, -0.75, 0.78, -3.32, 1.07]\nCurrent results : [316.64, 245.34, 186.72, 139.2, 101.31, 71.68, 49.04, 32.22, 20.16, 11.89, 6.54, 3.35, 1.66, 0.9, 0.61, 0.42, 0.08, -0.58, -1.61, -2.97, -4.53, -6.04, -7.15, -7.42, -6.31, -3.17, 2.74, 12.28, 26.38, 46.11]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 2000 - Cost : 102.11309449878976\nCurrent weights : [0.85, -0.77, 1.5, -3.31, 1.01]\nCurrent results : [314.82, 245.07, 187.53, 140.69, 103.17, 73.67, 50.97, 33.97, 21.65, 13.08, 7.44, 3.99, 2.09, 1.19, 0.85, 0.7, 0.49, 0.04, -0.72, -1.77, -3.0, -4.19, -5.04, -5.16, -4.05, -1.13, 4.28, 12.96, 25.79, 43.71]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 3000 - Cost : 78.8805699757381\nCurrent weights : [1.06, -0.8, 2.13, -3.29, 0.95]\nCurrent results : [313.23, 244.83, 188.23, 142.0, 104.81, 75.42, 52.67, 35.51, 22.96, 14.13, 8.23, 4.55, 2.46, 1.45, 1.06, 0.95, 0.84, 0.57, 0.04, -0.73, -1.67, -2.59, -3.21, -3.2, -2.09, 0.64, 5.62, 13.56, 25.27, 41.63]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 4000 - Cost : 61.1635840118118\nCurrent weights : [1.24, -0.83, 2.68, -3.28, 0.89]\nCurrent results : [311.82, 244.62, 188.85, 143.14, 106.24, 76.95, 54.16, 36.86, 24.11, 15.05, 8.92, 5.04, 2.79, 1.67, 1.24, 1.15, 1.14, 1.02, 0.7, 0.17, -0.52, -1.2, -1.63, -1.49, -0.39, 2.18, 6.79, 14.09, 24.82, 39.82]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 5000 - Cost : 47.642087265715524\nCurrent weights : [1.4, -0.86, 3.15, -3.27, 0.85]\nCurrent results : [310.59, 244.43, 189.39, 144.15, 107.5, 78.29, 55.47, 38.05, 25.12, 15.86, 9.53, 5.47, 3.08, 1.87, 1.4, 1.33, 1.4, 1.42, 1.27, 0.94, 0.48, 0.01, -0.25, -0.01, 1.1, 3.53, 7.81, 14.55, 24.43, 38.24]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 6000 - Cost : 37.31249994708311\nCurrent weights : [1.53, -0.9, 3.57, -3.25, 0.81]\nCurrent results : [309.51, 244.27, 189.86, 145.03, 108.6, 79.48, 56.62, 39.09, 26.01, 16.58, 10.07, 5.85, 3.34, 2.04, 1.53, 1.48, 1.62, 1.75, 1.76, 1.61, 1.33, 1.05, 0.94, 1.27, 2.38, 4.69, 8.69, 14.95, 24.1, 36.87]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 7000 - Cost : 29.41181313425119\nCurrent weights : [1.65, -0.94, 3.94, -3.24, 0.77]\nCurrent results : [308.55, 244.12, 190.27, 145.8, 109.57, 80.51, 57.63, 40.01, 26.79, 17.21, 10.55, 6.18, 3.56, 2.19, 1.65, 1.61, 1.81, 2.04, 2.18, 2.19, 2.08, 1.95, 1.97, 2.38, 3.5, 5.71, 9.46, 15.29, 23.81, 35.69]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 8000 - Cost : 23.359894293851138\nCurrent weights : [1.75, -0.98, 4.26, -3.23, 0.74]\nCurrent results : [307.71, 243.98, 190.63, 146.48, 110.42, 81.43, 58.53, 40.82, 27.49, 17.76, 10.97, 6.48, 3.76, 2.31, 1.75, 1.72, 1.97, 2.28, 2.54, 2.68, 2.72, 2.73, 2.87, 3.35, 4.47, 6.59, 10.13, 15.6, 23.56, 34.66]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 9000 - Cost : 18.715630671476152\nCurrent weights : [1.84, -1.02, 4.54, -3.22, 0.72]\nCurrent results : [306.97, 243.86, 190.95, 147.08, 111.18, 82.23, 59.31, 41.54, 28.1, 18.25, 11.34, 6.74, 3.93, 2.43, 1.84, 1.82, 2.1, 2.49, 2.84, 3.1, 3.27, 3.4, 3.64, 4.18, 5.31, 7.35, 10.71, 15.86, 23.35, 33.77]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 10000 - Cost : 15.14359150356463\nCurrent weights : [1.91, -1.07, 4.78, -3.22, 0.69]\nCurrent results : [306.31, 243.75, 191.22, 147.6, 111.84, 82.95, 60.01, 42.17, 28.64, 18.69, 11.67, 6.97, 4.08, 2.53, 1.91, 1.89, 2.21, 2.66, 3.11, 3.47, 3.74, 3.98, 4.3, 4.91, 6.04, 8.01, 11.22, 16.1, 23.16, 32.99]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 11000 - Cost : 12.388686087101608\nCurrent weights : [1.97, -1.11, 5.0, -3.21, 0.67]\nCurrent results : [305.73, 243.66, 191.47, 148.06, 112.42, 83.57, 60.62, 42.73, 29.12, 19.08, 11.96, 7.18, 4.22, 2.61, 1.97, 1.96, 2.31, 2.81, 3.33, 3.78, 4.14, 4.47, 4.88, 5.53, 6.67, 8.59, 11.66, 16.3, 23.0, 32.32]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 12000 - Cost : 10.256899885751997\nCurrent weights : [2.02, -1.15, 5.19, -3.2, 0.66]\nCurrent results : [305.21, 243.57, 191.68, 148.47, 112.93, 84.13, 61.16, 43.22, 29.54, 19.42, 12.22, 7.36, 4.34, 2.69, 2.02, 2.01, 2.38, 2.93, 3.51, 4.04, 4.49, 4.9, 5.37, 6.07, 7.21, 9.09, 12.04, 16.48, 22.87, 31.74]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n\nIteration 13000 - Cost : 8.600650847367833\nCurrent weights : [2.07, -1.2, 5.35, -3.19, 0.64]\nCurrent results : [304.75, 243.49, 191.87, 148.83, 113.39, 84.62, 61.65, 43.66, 29.92, 19.72, 12.45, 7.52, 4.44, 2.75, 2.07, 2.06, 2.45, 3.04, 3.67, 4.26, 4.79, 5.27, 5.8, 6.54, 7.69, 9.52, 12.37, 16.63, 22.75, 31.24]\nRequired results : [297.41, 243.46, 194.5, 153.17, 118.66, 90.17, 67.0, 48.46, 33.91, 22.77, 14.5, 8.61, 4.66, 2.24, 1.0, 0.64, 0.91, 1.58, 2.5, 3.55, 4.66, 5.8, 7.0, 8.33, 9.91, 11.89, 14.5, 17.99, 22.66, 28.86]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
